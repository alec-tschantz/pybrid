{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predcoding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho-Uzn-ZfKJn",
        "outputId": "8f2e172b-f18d-46d0-b61a-1584582396e6"
      },
      "source": [
        "#Â !pip install git+https://github.com/alec-tschantz/pybrid.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/alec-tschantz/pybrid.git\n",
            "  Cloning https://github.com/alec-tschantz/pybrid.git to /tmp/pip-req-build-z8cfr7ld\n",
            "  Running command git clone -q https://github.com/alec-tschantz/pybrid.git /tmp/pip-req-build-z8cfr7ld\n",
            "Requirement already satisfied (use --upgrade to upgrade): pybrid==0.0.1 from git+https://github.com/alec-tschantz/pybrid.git in /usr/local/lib/python3.7/dist-packages\n",
            "Building wheels for collected packages: pybrid\n",
            "  Building wheel for pybrid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybrid: filename=pybrid-0.0.1-cp37-none-any.whl size=8649 sha256=870accab5eb5649f71159a3495226ae2dfb04e694732ae5a228cf50cf04a81ab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ecl6esgw/wheels/7f/d2/e5/568382df15abbc70ecfd60b1864b418d0f1e39769d670f503b\n",
            "Successfully built pybrid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2XgNTz5hTkn"
      },
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "\n",
        "from pybrid import utils\n",
        "from pybrid import datasets\n",
        "from pybrid import optim\n",
        "from pybrid.models.hybrid import HybridModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcvB5JhPh7fm"
      },
      "source": [
        "def main(cfg):\n",
        "    cfg = utils.setup_experiment(cfg)\n",
        "\n",
        "    datasets.download_mnist()\n",
        "    train_dataset = datasets.MNIST(\n",
        "        train=True,\n",
        "        scale=cfg.data.label_scale,\n",
        "        size=cfg.data.train_size,\n",
        "        normalize=cfg.data.normalize,\n",
        "    )\n",
        "    test_dataset = datasets.MNIST(\n",
        "        train=False,\n",
        "        scale=cfg.data.label_scale,\n",
        "        size=cfg.data.test_size,\n",
        "        normalize=cfg.data.normalize,\n",
        "    )\n",
        "    train_loader = datasets.get_dataloader(train_dataset, cfg.optim.batch_size)\n",
        "    test_loader = datasets.get_dataloader(test_dataset, cfg.optim.batch_size)\n",
        "    msg = f\"loaded MNIST ({len(train_loader)} train batches {len(test_loader)} test batches)\"\n",
        "    logging.info(msg)\n",
        "\n",
        "    model = HybridModel(\n",
        "        nodes=cfg.model.nodes,\n",
        "        amort_nodes=cfg.model.amort_nodes,\n",
        "        mu_dt=cfg.infer.mu_dt,\n",
        "        act_fn=utils.get_act_fn(cfg.model.act_fn),\n",
        "        use_bias=cfg.model.use_bias,\n",
        "        kaiming_init=cfg.model.kaiming_init,\n",
        "    )\n",
        "    optimizer = optim.get_optim(\n",
        "        model.params,\n",
        "        cfg.optim.name,\n",
        "        cfg.optim.lr,\n",
        "        amort_lr=cfg.optim.amort_lr,\n",
        "        batch_scale=cfg.optim.batch_scale,\n",
        "        grad_clip=cfg.optim.grad_clip,\n",
        "        weight_decay=cfg.optim.weight_decay,\n",
        "    )\n",
        "    logging.info(f\"loaded model {model}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        metrics = {\"hybrid_acc\": [], \"pc_acc\": [], \"amort_acc\": []}\n",
        "        for epoch in range(1, cfg.exp.num_epochs + 1):\n",
        "            logging.info(f\"epoch {epoch}/{cfg.exp.num_epochs + 1}\")\n",
        "            pc_losses, amort_losses = [], []\n",
        "            logging.info(f\"Train @ epoch {epoch} ({len(train_loader)} batches)\")\n",
        "\n",
        "            for batch_id, (img_batch, label_batch) in enumerate(train_loader):\n",
        "                model.train_batch(\n",
        "                    img_batch,\n",
        "                    label_batch,\n",
        "                    cfg.infer.num_train_iters,\n",
        "                    fixed_preds=cfg.infer.fixed_preds_train,\n",
        "                    use_amort=cfg.model.train_amortised,\n",
        "                )\n",
        "                optimizer.step(\n",
        "                    curr_epoch=epoch,\n",
        "                    curr_batch=batch_id,\n",
        "                    n_batches=len(train_loader),\n",
        "                    batch_size=img_batch.size(0),\n",
        "                )\n",
        "\n",
        "                pc_loss, amort_loss = model.get_loss()\n",
        "                pc_losses.append(pc_loss)\n",
        "                amort_losses.append(amort_loss)\n",
        "\n",
        "                if batch_id % 100 == 0:\n",
        "                    pc_loss = sum(pc_losses) / (batch_id + 1)\n",
        "                    amort_loss = sum(amort_losses) / (batch_id + 1)\n",
        "                    msg = f\"[{batch_id}/{len(train_loader)}] pc loss {pc_loss:.4f} amort loss {amort_loss:.4f}\"\n",
        "                    logging.info(msg)\n",
        "\n",
        "            if epoch % cfg.exp.test_every == 0:\n",
        "                logging.info(f\"test @ epoch {epoch} ({len(test_loader)} batches)\")\n",
        "                pc_acc = 0\n",
        "                for _, (img_batch, label_batch) in enumerate(test_loader):\n",
        "                    label_preds = model.test_batch(\n",
        "                        img_batch,\n",
        "                        cfg.infer.num_test_iters,\n",
        "                        init_std=cfg.infer.init_std,\n",
        "                        fixed_preds=cfg.infer.fixed_preds_test,\n",
        "                        use_amort=False,\n",
        "                    )\n",
        "                    pc_acc = pc_acc + datasets.accuracy(label_preds, label_batch)                \n",
        "\n",
        "                pc_acc = pc_acc / len(test_loader)\n",
        "                metrics[\"pc_acc\"].append(pc_acc)\n",
        "                msg = \"pc acc {:.4f}\"\n",
        "                logging.info(msg.format(pc_acc))\n",
        "\n",
        "                _, label_batch = next(iter(test_loader))\n",
        "                img_preds = model.backward(label_batch)\n",
        "                datasets.plot_imgs(img_preds, cfg.exp.img_dir + f\"/{epoch}.png\")\n",
        "\n",
        "            if cfg.optim.normalize_weights:\n",
        "                model.normalize_weights()\n",
        "\n",
        "            utils.save_json(metrics, cfg.exp.log_dir + \"/metrics.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTwqMpFPiEj5",
        "outputId": "fc806205-e5ab-4912-ec7c-66cd3008103a"
      },
      "source": [
        "cfg = {\n",
        "    \"exp\": {\"log_dir\": \"results/predcoding\", \"seed\": 0, \"num_epochs\": 20, \"test_every\": 1},\n",
        "    \"data\": {\"train_size\": None, \"test_size\": None, \"label_scale\": 0.94, \"normalize\": True},\n",
        "    \"infer\": {\n",
        "        \"mu_dt\": 0.01,\n",
        "        \"num_train_iters\": 50,\n",
        "        \"num_test_iters\": 200,\n",
        "        \"fixed_preds_train\": False,\n",
        "        \"fixed_preds_test\": False,\n",
        "        \"init_std\": 0.01,\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"nodes\": [10, 500, 500, 784],\n",
        "        \"amort_nodes\": [784, 500, 500, 10],\n",
        "        \"train_amortised\": False,\n",
        "        \"use_bias\": True,\n",
        "        \"kaiming_init\": False,\n",
        "        \"act_fn\": \"tanh\",\n",
        "    },\n",
        "    \"optim\": {\n",
        "        \"name\": \"Adam\",\n",
        "        \"lr\": 1e-4,\n",
        "        \"amort_lr\": 1e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"batch_scale\": True,\n",
        "        \"grad_clip\": 5,\n",
        "        \"weight_decay\": None,\n",
        "        \"normalize_weights\": True\n",
        "    },\n",
        "}\n",
        "main(cfg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-22 13:00:55,063 [INFO] Starting experiment @ results/hybrid/0 [cuda]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'data': {'label_scale': 0.94,\n",
            "          'normalize': True,\n",
            "          'test_size': None,\n",
            "          'train_size': None},\n",
            " 'exp': {'img_dir': 'results/hybrid/0/imgs',\n",
            "         'log_dir': 'results/hybrid/0',\n",
            "         'num_epochs': 20,\n",
            "         'seed': 0,\n",
            "         'test_every': 1},\n",
            " 'infer': {'fixed_preds_test': False,\n",
            "           'fixed_preds_train': False,\n",
            "           'init_std': 0.01,\n",
            "           'mu_dt': 0.01,\n",
            "           'num_test_iters': 200,\n",
            "           'num_train_iters': 50},\n",
            " 'model': {'act_fn': 'tanh',\n",
            "           'amort_nodes': [784, 500, 500, 10],\n",
            "           'kaiming_init': False,\n",
            "           'nodes': [10, 500, 500, 784],\n",
            "           'train_amortised': True,\n",
            "           'use_bias': True},\n",
            " 'optim': {'amort_lr': 0.0001,\n",
            "           'batch_scale': True,\n",
            "           'batch_size': 64,\n",
            "           'grad_clip': 50,\n",
            "           'lr': 0.0001,\n",
            "           'name': 'Adam',\n",
            "           'weight_decay': None}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-22 13:01:02,188 [INFO] loaded MNIST (937 train batches 156 test batches)\n",
            "2021-03-22 13:01:02,207 [INFO] loaded model <HybridModel> [10, 500, 500, 784]\n",
            "2021-03-22 13:01:02,208 [INFO] \n",
            "epoch 1/21\n",
            "2021-03-22 13:01:02,208 [INFO] Train @ epoch 1 (937 batches)\n",
            "2021-03-22 13:01:02,243 [INFO] [0/937] pc loss 3255.7188 amort loss178.1057\n",
            "2021-03-22 13:01:04,739 [INFO] [100/937] pc loss 866.8825 amort loss77.3283\n",
            "2021-03-22 13:01:07,228 [INFO] [200/937] pc loss 616.4335 amort loss64.2682\n",
            "2021-03-22 13:01:09,706 [INFO] [300/937] pc loss 502.8942 amort loss57.7267\n",
            "2021-03-22 13:01:12,153 [INFO] [400/937] pc loss 434.8599 amort loss53.4731\n",
            "2021-03-22 13:01:14,630 [INFO] [500/937] pc loss 387.5079 amort loss50.3895\n",
            "2021-03-22 13:01:17,078 [INFO] [600/937] pc loss 352.5068 amort loss48.0020\n",
            "2021-03-22 13:01:19,544 [INFO] [700/937] pc loss 325.1014 amort loss46.1165\n",
            "2021-03-22 13:01:21,989 [INFO] [800/937] pc loss 302.8834 amort loss44.5506\n",
            "2021-03-22 13:01:24,441 [INFO] [900/937] pc loss 284.4593 amort loss43.2478\n",
            "2021-03-22 13:01:25,331 [INFO] test @ epoch 1 (156 batches)\n",
            "2021-03-22 13:02:01,046 [INFO] hybrid acc: 0.8493 pc acc 0.8509 amort acc 0.7657 \n",
            "2021-03-22 13:02:01,450 [INFO] \n",
            "epoch 2/21\n",
            "2021-03-22 13:02:01,451 [INFO] Train @ epoch 2 (937 batches)\n",
            "2021-03-22 13:02:01,483 [INFO] [0/937] pc loss 129.6721 amort loss29.4715\n",
            "2021-03-22 13:02:03,974 [INFO] [100/937] pc loss 124.4241 amort loss31.2917\n",
            "2021-03-22 13:02:06,428 [INFO] [200/937] pc loss 119.8795 amort loss31.0998\n",
            "2021-03-22 13:02:08,866 [INFO] [300/937] pc loss 116.2384 amort loss30.8153\n",
            "2021-03-22 13:02:11,342 [INFO] [400/937] pc loss 113.1328 amort loss30.5586\n",
            "2021-03-22 13:02:13,792 [INFO] [500/937] pc loss 109.8963 amort loss30.3754\n",
            "2021-03-22 13:02:16,266 [INFO] [600/937] pc loss 106.9885 amort loss30.1775\n",
            "2021-03-22 13:02:18,698 [INFO] [700/937] pc loss 104.2067 amort loss30.0488\n",
            "2021-03-22 13:02:21,167 [INFO] [800/937] pc loss 101.5949 amort loss29.8965\n",
            "2021-03-22 13:02:23,624 [INFO] [900/937] pc loss 99.1786 amort loss29.7735\n",
            "2021-03-22 13:02:24,512 [INFO] test @ epoch 2 (156 batches)\n",
            "2021-03-22 13:03:00,232 [INFO] hybrid acc: 0.8553 pc acc 0.8545 amort acc 0.8228 \n",
            "2021-03-22 13:03:00,631 [INFO] \n",
            "epoch 3/21\n",
            "2021-03-22 13:03:00,633 [INFO] Train @ epoch 3 (937 batches)\n",
            "2021-03-22 13:03:00,662 [INFO] [0/937] pc loss 76.5584 amort loss25.9206\n",
            "2021-03-22 13:03:03,181 [INFO] [100/937] pc loss 75.0270 amort loss28.1005\n",
            "2021-03-22 13:03:05,686 [INFO] [200/937] pc loss 72.9268 amort loss28.1155\n",
            "2021-03-22 13:03:08,152 [INFO] [300/937] pc loss 71.3257 amort loss28.0190\n",
            "2021-03-22 13:03:10,620 [INFO] [400/937] pc loss 69.9935 amort loss27.9083\n",
            "2021-03-22 13:03:13,172 [INFO] [500/937] pc loss 68.4722 amort loss27.8620\n",
            "2021-03-22 13:03:15,620 [INFO] [600/937] pc loss 67.1026 amort loss27.7826\n",
            "2021-03-22 13:03:18,091 [INFO] [700/937] pc loss 65.7182 amort loss27.7697\n",
            "2021-03-22 13:03:20,592 [INFO] [800/937] pc loss 64.4017 amort loss27.7144\n",
            "2021-03-22 13:03:23,070 [INFO] [900/937] pc loss 63.1659 amort loss27.6800\n",
            "2021-03-22 13:03:23,980 [INFO] test @ epoch 3 (156 batches)\n",
            "2021-03-22 13:04:00,013 [INFO] hybrid acc: 0.8557 pc acc 0.8523 amort acc 0.8384 \n",
            "2021-03-22 13:04:00,417 [INFO] \n",
            "epoch 4/21\n",
            "2021-03-22 13:04:00,418 [INFO] Train @ epoch 4 (937 batches)\n",
            "2021-03-22 13:04:00,448 [INFO] [0/937] pc loss 51.0268 amort loss24.7555\n",
            "2021-03-22 13:04:02,927 [INFO] [100/937] pc loss 50.5961 amort loss26.9014\n",
            "2021-03-22 13:04:05,383 [INFO] [200/937] pc loss 49.2804 amort loss26.9346\n",
            "2021-03-22 13:04:07,827 [INFO] [300/937] pc loss 48.3284 amort loss26.8813\n",
            "2021-03-22 13:04:10,264 [INFO] [400/937] pc loss 47.5661 amort loss26.7936\n",
            "2021-03-22 13:04:12,710 [INFO] [500/937] pc loss 46.6484 amort loss26.7792\n",
            "2021-03-22 13:04:15,154 [INFO] [600/937] pc loss 45.8329 amort loss26.7239\n",
            "2021-03-22 13:04:17,598 [INFO] [700/937] pc loss 44.9655 amort loss26.7399\n",
            "2021-03-22 13:04:20,034 [INFO] [800/937] pc loss 44.1469 amort loss26.7073\n",
            "2021-03-22 13:04:22,492 [INFO] [900/937] pc loss 43.3733 amort loss26.6952\n",
            "2021-03-22 13:04:23,395 [INFO] test @ epoch 4 (156 batches)\n",
            "2021-03-22 13:04:59,466 [INFO] hybrid acc: 0.8552 pc acc 0.8479 amort acc 0.8444 \n",
            "2021-03-22 13:04:59,857 [INFO] \n",
            "epoch 5/21\n",
            "2021-03-22 13:04:59,858 [INFO] Train @ epoch 5 (937 batches)\n",
            "2021-03-22 13:04:59,889 [INFO] [0/937] pc loss 35.5935 amort loss23.9606\n",
            "2021-03-22 13:05:02,435 [INFO] [100/937] pc loss 35.4783 amort loss26.1592\n",
            "2021-03-22 13:05:04,919 [INFO] [200/937] pc loss 34.5526 amort loss26.1941\n",
            "2021-03-22 13:05:07,424 [INFO] [300/937] pc loss 33.9153 amort loss26.1610\n",
            "2021-03-22 13:05:09,887 [INFO] [400/937] pc loss 33.4272 amort loss26.0772\n",
            "2021-03-22 13:05:12,376 [INFO] [500/937] pc loss 32.8240 amort loss26.0783\n",
            "2021-03-22 13:05:14,873 [INFO] [600/937] pc loss 32.3000 amort loss26.0330\n",
            "2021-03-22 13:05:17,358 [INFO] [700/937] pc loss 31.7128 amort loss26.0629\n",
            "2021-03-22 13:05:19,826 [INFO] [800/937] pc loss 31.1695 amort loss26.0378\n",
            "2021-03-22 13:05:22,284 [INFO] [900/937] pc loss 30.6547 amort loss26.0340\n",
            "2021-03-22 13:05:23,186 [INFO] test @ epoch 5 (156 batches)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}